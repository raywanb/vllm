{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5032f58367464a7d83b67a6806a9b280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "lora_conf = snapshot_download(repo_id=\"tybritten/lora-for-starcoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "lora_request=LoRARequest(\"testing adapter\", 1, lora_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-07 10:29:52 config.py:413] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 10:29:52,828\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.0.0.16:6379...\n",
      "2024-03-07 10:29:52,838\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-eu6m2d8n6xkuxbxenpykevvc1t.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2024-03-07 10:29:52,852\tINFO packaging.py:358 -- Pushing file package 'gcs://_ray_pkg_2d31ba67bd8cc395c3f62aaa48a4f17b.zip' (2.15MiB) to Ray cluster...\n",
      "2024-03-07 10:29:52,863\tINFO packaging.py:371 -- Successfully pushed file package 'gcs://_ray_pkg_2d31ba67bd8cc395c3f62aaa48a4f17b.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-07 10:29:54 llm_engine.py:79] Initializing an LLM engine with config: model='bigcode/starcoder', tokenizer='bigcode/starcoder', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "\u001b[36m(RayWorkerVllm pid=39007)\u001b[0m LoRAConfig(max_lora_rank=16, max_loras=1, max_cpu_loras=1, lora_dtype=torch.float16, lora_extra_vocab_size=256)\n",
      "LoRAConfig(max_lora_rank=16, max_loras=1, max_cpu_loras=1, lora_dtype=torch.float16, lora_extra_vocab_size=256)\n",
      "\u001b[36m(RayWorkerVllm pid=39007)\u001b[0m Unpadded vocab size:  49408\n",
      "\u001b[36m(RayWorkerVllm pid=39007)\u001b[0m Vocab size:  49152\n",
      "\u001b[36m(RayWorkerVllm pid=39007)\u001b[0m layers dict_keys(['lm_head_weight', 'transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.12.ln_1.weight', 'transformer.h.12.ln_1.bias', 'transformer.h.12.attn.c_attn.weight', 'transformer.h.12.attn.c_attn.bias', 'transformer.h.12.attn.c_proj.weight', 'transformer.h.12.attn.c_proj.bias', 'transformer.h.12.ln_2.weight', 'transformer.h.12.ln_2.bias', 'transformer.h.12.mlp.c_fc.weight', 'transformer.h.12.mlp.c_fc.bias', 'transformer.h.12.mlp.c_proj.weight', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.13.ln_1.weight', 'transformer.h.13.ln_1.bias', 'transformer.h.13.attn.c_attn.weight', 'transformer.h.13.attn.c_attn.bias', 'transformer.h.13.attn.c_proj.weight', 'transformer.h.13.attn.c_proj.bias', 'transformer.h.13.ln_2.weight', 'transformer.h.13.ln_2.bias', 'transformer.h.13.mlp.c_fc.weight', 'transformer.h.13.mlp.c_fc.bias', 'transformer.h.13.mlp.c_proj.weight', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.14.ln_1.weight', 'transformer.h.14.ln_1.bias', 'transformer.h.14.attn.c_attn.weight', 'transformer.h.14.attn.c_attn.bias', 'transformer.h.14.attn.c_proj.weight', 'transformer.h.14.attn.c_proj.bias', 'transformer.h.14.ln_2.weight', 'transformer.h.14.ln_2.bias', 'transformer.h.14.mlp.c_fc.weight', 'transformer.h.14.mlp.c_fc.bias', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.15.ln_1.weight', 'transformer.h.15.ln_1.bias', 'transformer.h.15.attn.c_attn.weight', 'transformer.h.15.attn.c_attn.bias', 'transformer.h.15.attn.c_proj.weight', 'transformer.h.15.attn.c_proj.bias', 'transformer.h.15.ln_2.weight', 'transformer.h.15.ln_2.bias', 'transformer.h.15.mlp.c_fc.weight', 'transformer.h.15.mlp.c_fc.bias', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.16.ln_1.weight', 'transformer.h.16.ln_1.bias', 'transformer.h.16.attn.c_attn.weight', 'transformer.h.16.attn.c_attn.bias', 'transformer.h.16.attn.c_proj.weight', 'transformer.h.16.attn.c_proj.bias', 'transformer.h.16.ln_2.weight', 'transformer.h.16.ln_2.bias', 'transformer.h.16.mlp.c_fc.weight', 'transformer.h.16.mlp.c_fc.bias', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.17.ln_1.weight', 'transformer.h.17.ln_1.bias', 'transformer.h.17.attn.c_attn.weight', 'transformer.h.17.attn.c_attn.bias', 'transformer.h.17.attn.c_proj.weight', 'transformer.h.17.attn.c_proj.bias', 'transformer.h.17.ln_2.weight', 'transformer.h.17.ln_2.bias', 'transformer.h.17.mlp.c_fc.weight', 'transformer.h.17.mlp.c_fc.bias', 'transformer.h.17.mlp.c_proj.weight', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.18.ln_1.weight', 'transformer.h.18.ln_1.bias', 'transformer.h.18.attn.c_attn.weight', 'transformer.h.18.attn.c_attn.bias', 'transformer.h.18.attn.c_proj.weight', 'transformer.h.18.attn.c_proj.bias', 'transformer.h.18.ln_2.weight', 'transformer.h.18.ln_2.bias', 'transformer.h.18.mlp.c_fc.weight', 'transformer.h.18.mlp.c_fc.bias', 'transformer.h.18.mlp.c_proj.weight', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.19.ln_1.weight', 'transformer.h.19.ln_1.bias', 'transformer.h.19.attn.c_attn.weight', 'transformer.h.19.attn.c_attn.bias', 'transformer.h.19.attn.c_proj.weight', 'transformer.h.19.attn.c_proj.bias', 'transformer.h.19.ln_2.weight', 'transformer.h.19.ln_2.bias', 'transformer.h.19.mlp.c_fc.weight', 'transformer.h.19.mlp.c_fc.bias', 'transformer.h.19.mlp.c_proj.weight', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.20.ln_1.weight', 'transformer.h.20.ln_1.bias', 'transformer.h.20.attn.c_attn.weight', 'transformer.h.20.attn.c_attn.bias', 'transformer.h.20.attn.c_proj.weight', 'transformer.h.20.attn.c_proj.bias', 'transformer.h.20.ln_2.weight', 'transformer.h.20.ln_2.bias', 'transformer.h.20.mlp.c_fc.weight', 'transformer.h.20.mlp.c_fc.bias', 'transformer.h.20.mlp.c_proj.weight', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.21.ln_1.weight', 'transformer.h.21.ln_1.bias', 'transformer.h.21.attn.c_attn.weight', 'transformer.h.21.attn.c_attn.bias', 'transformer.h.21.attn.c_proj.weight', 'transformer.h.21.attn.c_proj.bias', 'transformer.h.21.ln_2.weight', 'transformer.h.21.ln_2.bias', 'transformer.h.21.mlp.c_fc.weight', 'transformer.h.21.mlp.c_fc.bias', 'transformer.h.21.mlp.c_proj.weight', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.22.ln_1.weight', 'transformer.h.22.ln_1.bias', 'transformer.h.22.attn.c_attn.weight', 'transformer.h.22.attn.c_attn.bias', 'transformer.h.22.attn.c_proj.weight', 'transformer.h.22.attn.c_proj.bias', 'transformer.h.22.ln_2.weight', 'transformer.h.22.ln_2.bias', 'transformer.h.22.mlp.c_fc.weight', 'transformer.h.22.mlp.c_fc.bias', 'transformer.h.22.mlp.c_proj.weight', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.23.ln_1.weight', 'transformer.h.23.ln_1.bias', 'transformer.h.23.attn.c_attn.weight', 'transformer.h.23.attn.c_attn.bias', 'transformer.h.23.attn.c_proj.weight', 'transformer.h.23.attn.c_proj.bias', 'transformer.h.23.ln_2.weight', 'transformer.h.23.ln_2.bias', 'transformer.h.23.mlp.c_fc.weight', 'transformer.h.23.mlp.c_fc.bias', 'transformer.h.23.mlp.c_proj.weight', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.24.ln_1.weight', 'transformer.h.24.ln_1.bias', 'transformer.h.24.attn.c_attn.weight', 'transformer.h.24.attn.c_attn.bias', 'transformer.h.24.attn.c_proj.weight', 'transformer.h.24.attn.c_proj.bias', 'transformer.h.24.ln_2.weight', 'transformer.h.24.ln_2.bias', 'transformer.h.24.mlp.c_fc.weight', 'transformer.h.24.mlp.c_fc.bias', 'transformer.h.24.mlp.c_proj.weight', 'transformer.h.24.mlp.c_proj.bias', 'transformer.h.25.ln_1.weight', 'transformer.h.25.ln_1.bias', 'transformer.h.25.attn.c_attn.weight', 'transformer.h.25.attn.c_attn.bias', 'transformer.h.25.attn.c_proj.weight', 'transformer.h.25.attn.c_proj.bias', 'transformer.h.25.ln_2.weight', 'transformer.h.25.ln_2.bias', 'transformer.h.25.mlp.c_fc.weight', 'transformer.h.25.mlp.c_fc.bias', 'transformer.h.25.mlp.c_proj.weight', 'transformer.h.25.mlp.c_proj.bias', 'transformer.h.26.ln_1.weight', 'transformer.h.26.ln_1.bias', 'transformer.h.26.attn.c_attn.weight', 'transformer.h.26.attn.c_attn.bias', 'transformer.h.26.attn.c_proj.weight', 'transformer.h.26.attn.c_proj.bias', 'transformer.h.26.ln_2.weight', 'transformer.h.26.ln_2.bias', 'transformer.h.26.mlp.c_fc.weight', 'transformer.h.26.mlp.c_fc.bias', 'transformer.h.26.mlp.c_proj.weight', 'transformer.h.26.mlp.c_proj.bias', 'transformer.h.27.ln_1.weight', 'transformer.h.27.ln_1.bias', 'transformer.h.27.attn.c_attn.weight', 'transformer.h.27.attn.c_attn.bias', 'transformer.h.27.attn.c_proj.weight', 'transformer.h.27.attn.c_proj.bias', 'transformer.h.27.ln_2.weight', 'transformer.h.27.ln_2.bias', 'transformer.h.27.mlp.c_fc.weight', 'transformer.h.27.mlp.c_fc.bias', 'transformer.h.27.mlp.c_proj.weight', 'transformer.h.27.mlp.c_proj.bias', 'transformer.h.28.ln_1.weight', 'transformer.h.28.ln_1.bias', 'transformer.h.28.attn.c_attn.weight', 'transformer.h.28.attn.c_attn.bias', 'transformer.h.28.attn.c_proj.weight', 'transformer.h.28.attn.c_proj.bias', 'transformer.h.28.ln_2.weight', 'transformer.h.28.ln_2.bias', 'transformer.h.28.mlp.c_fc.weight', 'transformer.h.28.mlp.c_fc.bias', 'transformer.h.28.mlp.c_proj.weight', 'transformer.h.28.mlp.c_proj.bias', 'transformer.h.29.ln_1.weight', 'transformer.h.29.ln_1.bias', 'transformer.h.29.attn.c_attn.weight', 'transformer.h.29.attn.c_attn.bias', 'transformer.h.29.attn.c_proj.weight', 'transformer.h.29.attn.c_proj.bias', 'transformer.h.29.ln_2.weight', 'transformer.h.29.ln_2.bias', 'transformer.h.29.mlp.c_fc.weight', 'transformer.h.29.mlp.c_fc.bias', 'transformer.h.29.mlp.c_proj.weight', 'transformer.h.29.mlp.c_proj.bias', 'transformer.h.30.ln_1.weight', 'transformer.h.30.ln_1.bias', 'transformer.h.30.attn.c_attn.weight', 'transformer.h.30.attn.c_attn.bias', 'transformer.h.30.attn.c_proj.weight', 'transformer.h.30.attn.c_proj.bias', 'transformer.h.30.ln_2.weight', 'transformer.h.30.ln_2.bias', 'transformer.h.30.mlp.c_fc.weight', 'transformer.h.30.mlp.c_fc.bias', 'transformer.h.30.mlp.c_proj.weight', 'transformer.h.30.mlp.c_proj.bias', 'transformer.h.31.ln_1.weight', 'transformer.h.31.ln_1.bias', 'transformer.h.31.attn.c_attn.weight', 'transformer.h.31.attn.c_attn.bias', 'transformer.h.31.attn.c_proj.weight', 'transformer.h.31.attn.c_proj.bias', 'transformer.h.31.ln_2.weight', 'transformer.h.31.ln_2.bias', 'transformer.h.31.mlp.c_fc.weight', 'transformer.h.31.mlp.c_fc.bias', 'transformer.h.31.mlp.c_proj.weight', 'transformer.h.31.mlp.c_proj.bias', 'transformer.h.32.ln_1.weight', 'transformer.h.32.ln_1.bias', 'transformer.h.32.attn.c_attn.weight', 'transformer.h.32.attn.c_attn.bias', 'transformer.h.32.attn.c_proj.weight', 'transformer.h.32.attn.c_proj.bias', 'transformer.h.32.ln_2.weight', 'transformer.h.32.ln_2.bias', 'transformer.h.32.mlp.c_fc.weight', 'transformer.h.32.mlp.c_fc.bias', 'transformer.h.32.mlp.c_proj.weight', 'transformer.h.32.mlp.c_proj.bias', 'transformer.h.33.ln_1.weight', 'transformer.h.33.ln_1.bias', 'transformer.h.33.attn.c_attn.weight', 'transformer.h.33.attn.c_attn.bias', 'transformer.h.33.attn.c_proj.weight', 'transformer.h.33.attn.c_proj.bias', 'transformer.h.33.ln_2.weight', 'transformer.h.33.ln_2.bias', 'transformer.h.33.mlp.c_fc.weight', 'transformer.h.33.mlp.c_fc.bias', 'transformer.h.33.mlp.c_proj.weight', 'transformer.h.33.mlp.c_proj.bias', 'transformer.h.34.ln_1.weight', 'transformer.h.34.ln_1.bias', 'transformer.h.34.attn.c_attn.weight', 'transformer.h.34.attn.c_attn.bias', 'transformer.h.34.attn.c_proj.weight', 'transformer.h.34.attn.c_proj.bias', 'transformer.h.34.ln_2.weight', 'transformer.h.34.ln_2.bias', 'transformer.h.34.mlp.c_fc.weight', 'transformer.h.34.mlp.c_fc.bias', 'transformer.h.34.mlp.c_proj.weight', 'transformer.h.34.mlp.c_proj.bias', 'transformer.h.35.ln_1.weight', 'transformer.h.35.ln_1.bias', 'transformer.h.35.attn.c_attn.weight', 'transformer.h.35.attn.c_attn.bias', 'transformer.h.35.attn.c_proj.weight', 'transformer.h.35.attn.c_proj.bias', 'transformer.h.35.ln_2.weight', 'transformer.h.35.ln_2.bias', 'transformer.h.35.mlp.c_fc.weight', 'transformer.h.35.mlp.c_fc.bias', 'transformer.h.35.mlp.c_proj.weight', 'transformer.h.35.mlp.c_proj.bias', 'transformer.h.36.ln_1.weight', 'transformer.h.36.ln_1.bias', 'transformer.h.36.attn.c_attn.weight', 'transformer.h.36.attn.c_attn.bias', 'transformer.h.36.attn.c_proj.weight', 'transformer.h.36.attn.c_proj.bias', 'transformer.h.36.ln_2.weight', 'transformer.h.36.ln_2.bias', 'transformer.h.36.mlp.c_fc.weight', 'transformer.h.36.mlp.c_fc.bias', 'transformer.h.36.mlp.c_proj.weight', 'transformer.h.36.mlp.c_proj.bias', 'transformer.h.37.ln_1.weight', 'transformer.h.37.ln_1.bias', 'transformer.h.37.attn.c_attn.weight', 'transformer.h.37.attn.c_attn.bias', 'transformer.h.37.attn.c_proj.weight', 'transformer.h.37.attn.c_proj.bias', 'transformer.h.37.ln_2.weight', 'transformer.h.37.ln_2.bias', 'transformer.h.37.mlp.c_fc.weight', 'transformer.h.37.mlp.c_fc.bias', 'transformer.h.37.mlp.c_proj.weight', 'transformer.h.37.mlp.c_proj.bias', 'transformer.h.38.ln_1.weight', 'transformer.h.38.ln_1.bias', 'transformer.h.38.attn.c_attn.weight', 'transformer.h.38.attn.c_attn.bias', 'transformer.h.38.attn.c_proj.weight', 'transformer.h.38.attn.c_proj.bias', 'transformer.h.38.ln_2.weight', 'transformer.h.38.ln_2.bias', 'transformer.h.38.mlp.c_fc.weight', 'transformer.h.38.mlp.c_fc.bias', 'transformer.h.38.mlp.c_proj.weight', 'transformer.h.38.mlp.c_proj.bias', 'transformer.h.39.ln_1.weight', 'transformer.h.39.ln_1.bias', 'transformer.h.39.attn.c_attn.weight', 'transformer.h.39.attn.c_attn.bias', 'transformer.h.39.attn.c_proj.weight', 'transformer.h.39.attn.c_proj.bias', 'transformer.h.39.ln_2.weight', 'transformer.h.39.ln_2.bias', 'transformer.h.39.mlp.c_fc.weight', 'transformer.h.39.mlp.c_fc.bias', 'transformer.h.39.mlp.c_proj.weight', 'transformer.h.39.mlp.c_proj.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias'])\n",
      "Unpadded vocab size:  49408\n",
      "Vocab size:  49152\n",
      "layers dict_keys(['lm_head_weight', 'transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.12.ln_1.weight', 'transformer.h.12.ln_1.bias', 'transformer.h.12.attn.c_attn.weight', 'transformer.h.12.attn.c_attn.bias', 'transformer.h.12.attn.c_proj.weight', 'transformer.h.12.attn.c_proj.bias', 'transformer.h.12.ln_2.weight', 'transformer.h.12.ln_2.bias', 'transformer.h.12.mlp.c_fc.weight', 'transformer.h.12.mlp.c_fc.bias', 'transformer.h.12.mlp.c_proj.weight', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.13.ln_1.weight', 'transformer.h.13.ln_1.bias', 'transformer.h.13.attn.c_attn.weight', 'transformer.h.13.attn.c_attn.bias', 'transformer.h.13.attn.c_proj.weight', 'transformer.h.13.attn.c_proj.bias', 'transformer.h.13.ln_2.weight', 'transformer.h.13.ln_2.bias', 'transformer.h.13.mlp.c_fc.weight', 'transformer.h.13.mlp.c_fc.bias', 'transformer.h.13.mlp.c_proj.weight', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.14.ln_1.weight', 'transformer.h.14.ln_1.bias', 'transformer.h.14.attn.c_attn.weight', 'transformer.h.14.attn.c_attn.bias', 'transformer.h.14.attn.c_proj.weight', 'transformer.h.14.attn.c_proj.bias', 'transformer.h.14.ln_2.weight', 'transformer.h.14.ln_2.bias', 'transformer.h.14.mlp.c_fc.weight', 'transformer.h.14.mlp.c_fc.bias', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.15.ln_1.weight', 'transformer.h.15.ln_1.bias', 'transformer.h.15.attn.c_attn.weight', 'transformer.h.15.attn.c_attn.bias', 'transformer.h.15.attn.c_proj.weight', 'transformer.h.15.attn.c_proj.bias', 'transformer.h.15.ln_2.weight', 'transformer.h.15.ln_2.bias', 'transformer.h.15.mlp.c_fc.weight', 'transformer.h.15.mlp.c_fc.bias', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.16.ln_1.weight', 'transformer.h.16.ln_1.bias', 'transformer.h.16.attn.c_attn.weight', 'transformer.h.16.attn.c_attn.bias', 'transformer.h.16.attn.c_proj.weight', 'transformer.h.16.attn.c_proj.bias', 'transformer.h.16.ln_2.weight', 'transformer.h.16.ln_2.bias', 'transformer.h.16.mlp.c_fc.weight', 'transformer.h.16.mlp.c_fc.bias', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.17.ln_1.weight', 'transformer.h.17.ln_1.bias', 'transformer.h.17.attn.c_attn.weight', 'transformer.h.17.attn.c_attn.bias', 'transformer.h.17.attn.c_proj.weight', 'transformer.h.17.attn.c_proj.bias', 'transformer.h.17.ln_2.weight', 'transformer.h.17.ln_2.bias', 'transformer.h.17.mlp.c_fc.weight', 'transformer.h.17.mlp.c_fc.bias', 'transformer.h.17.mlp.c_proj.weight', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.18.ln_1.weight', 'transformer.h.18.ln_1.bias', 'transformer.h.18.attn.c_attn.weight', 'transformer.h.18.attn.c_attn.bias', 'transformer.h.18.attn.c_proj.weight', 'transformer.h.18.attn.c_proj.bias', 'transformer.h.18.ln_2.weight', 'transformer.h.18.ln_2.bias', 'transformer.h.18.mlp.c_fc.weight', 'transformer.h.18.mlp.c_fc.bias', 'transformer.h.18.mlp.c_proj.weight', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.19.ln_1.weight', 'transformer.h.19.ln_1.bias', 'transformer.h.19.attn.c_attn.weight', 'transformer.h.19.attn.c_attn.bias', 'transformer.h.19.attn.c_proj.weight', 'transformer.h.19.attn.c_proj.bias', 'transformer.h.19.ln_2.weight', 'transformer.h.19.ln_2.bias', 'transformer.h.19.mlp.c_fc.weight', 'transformer.h.19.mlp.c_fc.bias', 'transformer.h.19.mlp.c_proj.weight', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.20.ln_1.weight', 'transformer.h.20.ln_1.bias', 'transformer.h.20.attn.c_attn.weight', 'transformer.h.20.attn.c_attn.bias', 'transformer.h.20.attn.c_proj.weight', 'transformer.h.20.attn.c_proj.bias', 'transformer.h.20.ln_2.weight', 'transformer.h.20.ln_2.bias', 'transformer.h.20.mlp.c_fc.weight', 'transformer.h.20.mlp.c_fc.bias', 'transformer.h.20.mlp.c_proj.weight', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.21.ln_1.weight', 'transformer.h.21.ln_1.bias', 'transformer.h.21.attn.c_attn.weight', 'transformer.h.21.attn.c_attn.bias', 'transformer.h.21.attn.c_proj.weight', 'transformer.h.21.attn.c_proj.bias', 'transformer.h.21.ln_2.weight', 'transformer.h.21.ln_2.bias', 'transformer.h.21.mlp.c_fc.weight', 'transformer.h.21.mlp.c_fc.bias', 'transformer.h.21.mlp.c_proj.weight', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.22.ln_1.weight', 'transformer.h.22.ln_1.bias', 'transformer.h.22.attn.c_attn.weight', 'transformer.h.22.attn.c_attn.bias', 'transformer.h.22.attn.c_proj.weight', 'transformer.h.22.attn.c_proj.bias', 'transformer.h.22.ln_2.weight', 'transformer.h.22.ln_2.bias', 'transformer.h.22.mlp.c_fc.weight', 'transformer.h.22.mlp.c_fc.bias', 'transformer.h.22.mlp.c_proj.weight', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.23.ln_1.weight', 'transformer.h.23.ln_1.bias', 'transformer.h.23.attn.c_attn.weight', 'transformer.h.23.attn.c_attn.bias', 'transformer.h.23.attn.c_proj.weight', 'transformer.h.23.attn.c_proj.bias', 'transformer.h.23.ln_2.weight', 'transformer.h.23.ln_2.bias', 'transformer.h.23.mlp.c_fc.weight', 'transformer.h.23.mlp.c_fc.bias', 'transformer.h.23.mlp.c_proj.weight', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.24.ln_1.weight', 'transformer.h.24.ln_1.bias', 'transformer.h.24.attn.c_attn.weight', 'transformer.h.24.attn.c_attn.bias', 'transformer.h.24.attn.c_proj.weight', 'transformer.h.24.attn.c_proj.bias', 'transformer.h.24.ln_2.weight', 'transformer.h.24.ln_2.bias', 'transformer.h.24.mlp.c_fc.weight', 'transformer.h.24.mlp.c_fc.bias', 'transformer.h.24.mlp.c_proj.weight', 'transformer.h.24.mlp.c_proj.bias', 'transformer.h.25.ln_1.weight', 'transformer.h.25.ln_1.bias', 'transformer.h.25.attn.c_attn.weight', 'transformer.h.25.attn.c_attn.bias', 'transformer.h.25.attn.c_proj.weight', 'transformer.h.25.attn.c_proj.bias', 'transformer.h.25.ln_2.weight', 'transformer.h.25.ln_2.bias', 'transformer.h.25.mlp.c_fc.weight', 'transformer.h.25.mlp.c_fc.bias', 'transformer.h.25.mlp.c_proj.weight', 'transformer.h.25.mlp.c_proj.bias', 'transformer.h.26.ln_1.weight', 'transformer.h.26.ln_1.bias', 'transformer.h.26.attn.c_attn.weight', 'transformer.h.26.attn.c_attn.bias', 'transformer.h.26.attn.c_proj.weight', 'transformer.h.26.attn.c_proj.bias', 'transformer.h.26.ln_2.weight', 'transformer.h.26.ln_2.bias', 'transformer.h.26.mlp.c_fc.weight', 'transformer.h.26.mlp.c_fc.bias', 'transformer.h.26.mlp.c_proj.weight', 'transformer.h.26.mlp.c_proj.bias', 'transformer.h.27.ln_1.weight', 'transformer.h.27.ln_1.bias', 'transformer.h.27.attn.c_attn.weight', 'transformer.h.27.attn.c_attn.bias', 'transformer.h.27.attn.c_proj.weight', 'transformer.h.27.attn.c_proj.bias', 'transformer.h.27.ln_2.weight', 'transformer.h.27.ln_2.bias', 'transformer.h.27.mlp.c_fc.weight', 'transformer.h.27.mlp.c_fc.bias', 'transformer.h.27.mlp.c_proj.weight', 'transformer.h.27.mlp.c_proj.bias', 'transformer.h.28.ln_1.weight', 'transformer.h.28.ln_1.bias', 'transformer.h.28.attn.c_attn.weight', 'transformer.h.28.attn.c_attn.bias', 'transformer.h.28.attn.c_proj.weight', 'transformer.h.28.attn.c_proj.bias', 'transformer.h.28.ln_2.weight', 'transformer.h.28.ln_2.bias', 'transformer.h.28.mlp.c_fc.weight', 'transformer.h.28.mlp.c_fc.bias', 'transformer.h.28.mlp.c_proj.weight', 'transformer.h.28.mlp.c_proj.bias', 'transformer.h.29.ln_1.weight', 'transformer.h.29.ln_1.bias', 'transformer.h.29.attn.c_attn.weight', 'transformer.h.29.attn.c_attn.bias', 'transformer.h.29.attn.c_proj.weight', 'transformer.h.29.attn.c_proj.bias', 'transformer.h.29.ln_2.weight', 'transformer.h.29.ln_2.bias', 'transformer.h.29.mlp.c_fc.weight', 'transformer.h.29.mlp.c_fc.bias', 'transformer.h.29.mlp.c_proj.weight', 'transformer.h.29.mlp.c_proj.bias', 'transformer.h.30.ln_1.weight', 'transformer.h.30.ln_1.bias', 'transformer.h.30.attn.c_attn.weight', 'transformer.h.30.attn.c_attn.bias', 'transformer.h.30.attn.c_proj.weight', 'transformer.h.30.attn.c_proj.bias', 'transformer.h.30.ln_2.weight', 'transformer.h.30.ln_2.bias', 'transformer.h.30.mlp.c_fc.weight', 'transformer.h.30.mlp.c_fc.bias', 'transformer.h.30.mlp.c_proj.weight', 'transformer.h.30.mlp.c_proj.bias', 'transformer.h.31.ln_1.weight', 'transformer.h.31.ln_1.bias', 'transformer.h.31.attn.c_attn.weight', 'transformer.h.31.attn.c_attn.bias', 'transformer.h.31.attn.c_proj.weight', 'transformer.h.31.attn.c_proj.bias', 'transformer.h.31.ln_2.weight', 'transformer.h.31.ln_2.bias', 'transformer.h.31.mlp.c_fc.weight', 'transformer.h.31.mlp.c_fc.bias', 'transformer.h.31.mlp.c_proj.weight', 'transformer.h.31.mlp.c_proj.bias', 'transformer.h.32.ln_1.weight', 'transformer.h.32.ln_1.bias', 'transformer.h.32.attn.c_attn.weight', 'transformer.h.32.attn.c_attn.bias', 'transformer.h.32.attn.c_proj.weight', 'transformer.h.32.attn.c_proj.bias', 'transformer.h.32.ln_2.weight', 'transformer.h.32.ln_2.bias', 'transformer.h.32.mlp.c_fc.weight', 'transformer.h.32.mlp.c_fc.bias', 'transformer.h.32.mlp.c_proj.weight', 'transformer.h.32.mlp.c_proj.bias', 'transformer.h.33.ln_1.weight', 'transformer.h.33.ln_1.bias', 'transformer.h.33.attn.c_attn.weight', 'transformer.h.33.attn.c_attn.bias', 'transformer.h.33.attn.c_proj.weight', 'transformer.h.33.attn.c_proj.bias', 'transformer.h.33.ln_2.weight', 'transformer.h.33.ln_2.bias', 'transformer.h.33.mlp.c_fc.weight', 'transformer.h.33.mlp.c_fc.bias', 'transformer.h.33.mlp.c_proj.weight', 'transformer.h.33.mlp.c_proj.bias', 'transformer.h.34.ln_1.weight', 'transformer.h.34.ln_1.bias', 'transformer.h.34.attn.c_attn.weight', 'transformer.h.34.attn.c_attn.bias', 'transformer.h.34.attn.c_proj.weight', 'transformer.h.34.attn.c_proj.bias', 'transformer.h.34.ln_2.weight', 'transformer.h.34.ln_2.bias', 'transformer.h.34.mlp.c_fc.weight', 'transformer.h.34.mlp.c_fc.bias', 'transformer.h.34.mlp.c_proj.weight', 'transformer.h.34.mlp.c_proj.bias', 'transformer.h.35.ln_1.weight', 'transformer.h.35.ln_1.bias', 'transformer.h.35.attn.c_attn.weight', 'transformer.h.35.attn.c_attn.bias', 'transformer.h.35.attn.c_proj.weight', 'transformer.h.35.attn.c_proj.bias', 'transformer.h.35.ln_2.weight', 'transformer.h.35.ln_2.bias', 'transformer.h.35.mlp.c_fc.weight', 'transformer.h.35.mlp.c_fc.bias', 'transformer.h.35.mlp.c_proj.weight', 'transformer.h.35.mlp.c_proj.bias', 'transformer.h.36.ln_1.weight', 'transformer.h.36.ln_1.bias', 'transformer.h.36.attn.c_attn.weight', 'transformer.h.36.attn.c_attn.bias', 'transformer.h.36.attn.c_proj.weight', 'transformer.h.36.attn.c_proj.bias', 'transformer.h.36.ln_2.weight', 'transformer.h.36.ln_2.bias', 'transformer.h.36.mlp.c_fc.weight', 'transformer.h.36.mlp.c_fc.bias', 'transformer.h.36.mlp.c_proj.weight', 'transformer.h.36.mlp.c_proj.bias', 'transformer.h.37.ln_1.weight', 'transformer.h.37.ln_1.bias', 'transformer.h.37.attn.c_attn.weight', 'transformer.h.37.attn.c_attn.bias', 'transformer.h.37.attn.c_proj.weight', 'transformer.h.37.attn.c_proj.bias', 'transformer.h.37.ln_2.weight', 'transformer.h.37.ln_2.bias', 'transformer.h.37.mlp.c_fc.weight', 'transformer.h.37.mlp.c_fc.bias', 'transformer.h.37.mlp.c_proj.weight', 'transformer.h.37.mlp.c_proj.bias', 'transformer.h.38.ln_1.weight', 'transformer.h.38.ln_1.bias', 'transformer.h.38.attn.c_attn.weight', 'transformer.h.38.attn.c_attn.bias', 'transformer.h.38.attn.c_proj.weight', 'transformer.h.38.attn.c_proj.bias', 'transformer.h.38.ln_2.weight', 'transformer.h.38.ln_2.bias', 'transformer.h.38.mlp.c_fc.weight', 'transformer.h.38.mlp.c_fc.bias', 'transformer.h.38.mlp.c_proj.weight', 'transformer.h.38.mlp.c_proj.bias', 'transformer.h.39.ln_1.weight', 'transformer.h.39.ln_1.bias', 'transformer.h.39.attn.c_attn.weight', 'transformer.h.39.attn.c_attn.bias', 'transformer.h.39.attn.c_proj.weight', 'transformer.h.39.attn.c_proj.bias', 'transformer.h.39.ln_2.weight', 'transformer.h.39.ln_2.bias', 'transformer.h.39.mlp.c_fc.weight', 'transformer.h.39.mlp.c_fc.bias', 'transformer.h.39.mlp.c_proj.weight', 'transformer.h.39.mlp.c_proj.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias'])\n",
      "INFO 03-07 10:30:08 weight_utils.py:163] Using model weights format ['*.bin']\n",
      "\u001b[36m(RayWorkerVllm pid=39007)\u001b[0m INFO 03-07 10:30:08 weight_utils.py:163] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No suitable kernel. h_in=16 h_out=6144 dtype=Half out_dtype=Half",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbigcode/starcoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_lora\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/entrypoints/llm.py:109\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_log_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     91\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m     92\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     93\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    108\u001b[0m )\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/engine/llm_engine.py:375\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args)\u001b[0m\n\u001b[1;32m    373\u001b[0m placement_group \u001b[38;5;241m=\u001b[39m initialize_cluster(parallel_config)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m             \u001b[49m\u001b[43mplacement_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m             \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/engine/llm_engine.py:123\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, lora_config, placement_group, log_stats)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_workers()\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Profile the memory usage and initialize the cache.\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Create the scheduler.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m Scheduler(scheduler_config, cache_config, lora_config)\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/engine/llm_engine.py:327\u001b[0m, in \u001b[0;36mLLMEngine._init_cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m\"\"\"Profiles the memory usage and initializes the KV cache.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03mThe engine will first conduct a profiling of the existing memory usage.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m    by adjusting the `gpu_memory_utilization` parameters.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# Get the maximum number of blocks that can be allocated on GPU and CPU.\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m num_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprofile_num_available_blocks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcpu_swap_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswap_space_bytes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Since we use a shared centralized controller, we take the minimum\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# number of blocks across all workers to make sure all the memory\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# operators can be applied to all workers.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m num_gpu_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(b[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m num_blocks)\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/engine/llm_engine.py:1021\u001b[0m, in \u001b[0;36mLLMEngine._run_workers\u001b[0;34m(self, method, driver_args, driver_kwargs, max_concurrent_workers, use_ray_compiled_dag, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     driver_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# Start the driver worker after all the ray workers.\u001b[39;00m\n\u001b[0;32m-> 1021\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# Get the results of the ray workers.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/worker/worker.py:124\u001b[0m, in \u001b[0;36mWorker.profile_num_available_blocks\u001b[0;34m(self, block_size, gpu_memory_utilization, cpu_swap_space, cache_dtype)\u001b[0m\n\u001b[1;32m    120\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Execute a forward pass with dummy inputs to profile the memory usage\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# of the model.\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofile_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Calculate the number of blocks that can be allocated with the\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# profiled peak memory.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/worker/model_runner.py:638\u001b[0m, in \u001b[0;36mModelRunner.profile_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    636\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mget_num_layers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config)\n\u001b[1;32m    637\u001b[0m kv_caches \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)] \u001b[38;5;241m*\u001b[39m num_layers\n\u001b[0;32m--> 638\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/worker/model_runner.py:573\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, seq_group_metadata_list, kv_caches)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     model_executable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m--> 573\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_executable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[1;32m    581\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    582\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    583\u001b[0m     sampling_metadata\u001b[38;5;241m=\u001b[39msampling_metadata,\n\u001b[1;32m    584\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/model_executor/models/gpt_bigcode.py:281\u001b[0m, in \u001b[0;36mGPTBigCodeForCausalLM.forward\u001b[0;34m(self, input_ids, positions, kv_caches, input_metadata)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m     input_metadata: InputMetadata,\n\u001b[1;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 281\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43minput_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/model_executor/models/gpt_bigcode.py:220\u001b[0m, in \u001b[0;36mGPTBigCodeModel.forward\u001b[0;34m(self, input_ids, position_ids, kv_caches, input_metadata)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    215\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     input_metadata: InputMetadata,\n\u001b[1;32m    219\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 220\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe(position_ids)\n\u001b[1;32m    222\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/lora/layers.py:276\u001b[0m, in \u001b[0;36mVocabParallelEmbeddingWithLoRA.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_lora_a_embeddings\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    273\u001b[0m     full_lora_a_embeddings \u001b[38;5;241m=\u001b[39m full_lora_a_embeddings\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    274\u001b[0m         full_lora_a_embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    275\u001b[0m         full_lora_a_embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 276\u001b[0m \u001b[43mbgmv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_lora_a_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_b_stacked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices_len\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m full_output\u001b[38;5;241m.\u001b[39mview_as(full_output_org)\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/lora/punica.py:49\u001b[0m, in \u001b[0;36mbgmv\u001b[0;34m(y, x, w_t_all, indicies, layer_idx, scale)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     47\u001b[0m     _raise_import_error(e)\n\u001b[0;32m---> 49\u001b[0m \u001b[43mpunica_kernels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_bgmv\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_t_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindicies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No suitable kernel. h_in=16 h_out=6144 dtype=Half out_dtype=Half"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 10:31:11,957\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RayWorkerVllm.execute_method()\u001b[39m (pid=39007, ip=10.0.0.16, actor_id=4a0e7bd891e7c6b21a0481ab09000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7f41916fc6d0>)\n",
      "  File \"/tmp/ray/session_2024-03-07_09-12-15_688256_5067/runtime_resources/working_dir_files/_ray_pkg_2d31ba67bd8cc395c3f62aaa48a4f17b/vllm/engine/ray_utils.py\", line 37, in execute_method\n",
      "    return executor(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/ray/session_2024-03-07_09-12-15_688256_5067/runtime_resources/working_dir_files/_ray_pkg_2d31ba67bd8cc395c3f62aaa48a4f17b/vllm/worker/worker.py\", line 124, in profile_num_available_blocks\n",
      "    self.model_runner.profile_run()\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/ray/session_2024-03-07_09-12-15_688256_5067/runtime_resources/working_dir_files/_ray_pkg_2d31ba67bd8cc395c3f62aaa48a4f17b/vllm/worker/model_runner.py\", line 638, in profile_run\n",
      "    self.execute_model(seqs, kv_caches)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/ray/session_2024-03-07_09-12-15_688256_5067/runtime_resources/working_dir_files/_ray_pkg_2d31ba67bd8cc395c3f62aaa48a4f17b/vllm/worker/model_runner.py\", line 573, in execute_model\n",
      "    hidden_states = model_executable(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ray/session_2024-03-07_09-12-15_688256_5067/runtime_resources/working_dir_files/_ray_pkg_2d31ba67bd8cc395c3f62aaa48a4f17b/vllm/model_executor/models/gpt_bigcode.py\", line 281, in forward\n",
      "    hidden_states = self.transformer(input_ids, positions, kv_caches,\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ray/session_2024-03-07_09-12-15_688256_5067/runtime_resources/working_dir_files/_ray_pkg_2d31ba67bd8cc395c3f62aaa48a4f17b/vllm/model_executor/models/gpt_bigcode.py\", line 220, in forward\n",
      "    inputs_embeds = self.wte(input_ids)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ray/session_2024-03-07_09-12-15_688256_5067/runtime_resources/working_dir_files/_ray_pkg_2d31ba67bd8cc395c3f62aaa48a4f17b/vllm/lora/layers.py\", line 276, in forward\n",
      "    bgmv(full_output, full_lora_a_embeddings, self.lora_b_stacked,\n",
      "  File \"/tmp/ray/session_2024-03-07_09-12-15_688256_5067/runtime_resources/working_dir_files/_ray_pkg_2d31ba67bd8cc395c3f62aaa48a4f17b/vllm/lora/punica.py\", line 49, in bgmv\n",
      "    punica_kernels.dispatch_bgmv(y, x, w_t_all, indicies, layer_idx, scale)\n",
      "RuntimeError: No suitable kernel. h_in=16 h_out=6144 dtype=Half out_dtype=Half\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"bigcode/starcoder\", tensor_parallel_size=2, enable_lora=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
