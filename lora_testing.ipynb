{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2756d68e474658831097a098d54891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cc419f042b4ab19dde372a45e074ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitignore:   0%|          | 0.00/13.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9badc8133e9d45879e14ff5c77a7f9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95b67a592b44c668b5ddcf3b719f9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "final_checkpoint/adapter_config.json:   0%|          | 0.00/478 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1e7f26587e47f086db1731c2e0faa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "final_checkpoint/README.md:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93ed338feae443cb16c6aad793c5417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/55.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee15a7d9c69f417d8332536f70e8a35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/478 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c92b678ca7e84813bb170b915b9623ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65c40b1f0d14d60b07627e06b6b4d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66814d62112f4b80b5f6669a9d4d9fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/55.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "lora_conf = snapshot_download(repo_id=\"KarthiAru/peft-lora-starcoder-personal-copilot-A100-40GB-colab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b3ba658c7f48d4953ca20837fe6f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "lora_request=LoRARequest(\"testing adapter\", 1, lora_conf)\n",
    "lora_request.lora_local_path\n",
    "import torch\n",
    "\n",
    "lora_state_dict = torch.load(lora_request.lora_local_path+'/adapter_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lora_state_dict['base_model.model.transformer.h.0.attn.c_attn.lora_A.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e75fc0c575749acb0bfbdb13735ff4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-11 23:43:04 config.py:413] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 23:43:05,301\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 10.0.0.30:6379...\n",
      "2024-03-11 23:43:05,312\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-eu6m2d8n6xkuxbxenpykevvc1t.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2024-03-11 23:43:05,327\tINFO packaging.py:358 -- Pushing file package 'gcs://_ray_pkg_6ae155751569d798e67bbac55e5b93fd.zip' (2.27MiB) to Ray cluster...\n",
      "2024-03-11 23:43:05,337\tINFO packaging.py:371 -- Successfully pushed file package 'gcs://_ray_pkg_6ae155751569d798e67bbac55e5b93fd.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-11 23:43:06 llm_engine.py:79] Initializing an LLM engine with config: model='bigcode/starcoder', tokenizer='bigcode/starcoder', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=4, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9303a25744d2412ca7392e018ee1b294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/677 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbdced2d95624d168c6e0861a200032e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/777k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bc67142a1a4403afa2efa38347dff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21901a0514444d4e8677b8373a90d534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb39a769c7646019cdac504cbcc44aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/532 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRAConfig(max_lora_rank=16, max_loras=1, max_cpu_loras=1, lora_dtype=torch.float16, lora_extra_vocab_size=256)\n",
      "\u001b[36m(RayWorkerVllm pid=14240)\u001b[0m LoRAConfig(max_lora_rank=16, max_loras=1, max_cpu_loras=1, lora_dtype=torch.float16, lora_extra_vocab_size=256)\n",
      "Unpadded vocab size:  49408\n",
      "Vocab size:  49152\n",
      "\u001b[36m(RayWorkerVllm pid=14240)\u001b[0m Unpadded vocab size:  49408\n",
      "\u001b[36m(RayWorkerVllm pid=14240)\u001b[0m Vocab size:  49152\n",
      "INFO 03-11 23:43:30 weight_utils.py:163] Using model weights format ['*.bin']\n",
      "\u001b[36m(RayWorkerVllm pid=14240)\u001b[0m INFO 03-11 23:43:30 weight_utils.py:163] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c11cd79eafc46d9a2fcc95aab39d589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00007.bin:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d59c1a6eae4a2d85b875638f50f3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00007.bin:   0%|          | 0.00/9.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13efc13343ca420989fe2f798ce842f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00006-of-00007.bin:   0%|          | 0.00/9.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e756f16706c4c2fb8c803ee007e2998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00007.bin:   0%|          | 0.00/9.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836ec2eeb24f4b999565df8a52b2574e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00007-of-00007.bin:   0%|          | 0.00/4.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888dda9181a74098b9ebd585e7449079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00005-of-00007.bin:   0%|          | 0.00/9.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919a94f395ad43b1a9619213cb7ce028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00007.bin:   0%|          | 0.00/9.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +1m10s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbigcode/starcoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_lora\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/entrypoints/llm.py:109\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_log_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     91\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m     92\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     93\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    108\u001b[0m )\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/engine/llm_engine.py:375\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args)\u001b[0m\n\u001b[1;32m    373\u001b[0m placement_group \u001b[38;5;241m=\u001b[39m initialize_cluster(parallel_config)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m             \u001b[49m\u001b[43mplacement_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m             \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/engine/llm_engine.py:123\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, lora_config, placement_group, log_stats)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_workers()\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Profile the memory usage and initialize the cache.\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Create the scheduler.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m Scheduler(scheduler_config, cache_config, lora_config)\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/engine/llm_engine.py:327\u001b[0m, in \u001b[0;36mLLMEngine._init_cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m\"\"\"Profiles the memory usage and initializes the KV cache.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03mThe engine will first conduct a profiling of the existing memory usage.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m    by adjusting the `gpu_memory_utilization` parameters.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# Get the maximum number of blocks that can be allocated on GPU and CPU.\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m num_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprofile_num_available_blocks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcpu_swap_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswap_space_bytes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Since we use a shared centralized controller, we take the minimum\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# number of blocks across all workers to make sure all the memory\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# operators can be applied to all workers.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m num_gpu_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(b[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m num_blocks)\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/engine/llm_engine.py:1021\u001b[0m, in \u001b[0;36mLLMEngine._run_workers\u001b[0;34m(self, method, driver_args, driver_kwargs, max_concurrent_workers, use_ray_compiled_dag, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     driver_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# Start the driver worker after all the ray workers.\u001b[39;00m\n\u001b[0;32m-> 1021\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# Get the results of the ray workers.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/worker/worker.py:124\u001b[0m, in \u001b[0;36mWorker.profile_num_available_blocks\u001b[0;34m(self, block_size, gpu_memory_utilization, cpu_swap_space, cache_dtype)\u001b[0m\n\u001b[1;32m    120\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Execute a forward pass with dummy inputs to profile the memory usage\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# of the model.\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofile_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Calculate the number of blocks that can be allocated with the\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# profiled peak memory.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/worker/model_runner.py:609\u001b[0m, in \u001b[0;36mModelRunner.profile_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    603\u001b[0m     lora_id \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    604\u001b[0m     dummy_lora_request \u001b[38;5;241m=\u001b[39m LoRARequest(\n\u001b[1;32m    605\u001b[0m         lora_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarmup_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlora_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    606\u001b[0m         lora_int_id\u001b[38;5;241m=\u001b[39mlora_id,\n\u001b[1;32m    607\u001b[0m         lora_local_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/not/a/real/path\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[0;32m--> 609\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_dummy_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_lora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLORA_WARMUP_RANK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m     dummy_lora_requests\u001b[38;5;241m.\u001b[39mappend(dummy_lora_request)\n\u001b[1;32m    612\u001b[0m dummy_lora_requests_per_seq \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    613\u001b[0m     dummy_lora_requests[idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(dummy_lora_requests)]\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_num_seqs)\n\u001b[1;32m    615\u001b[0m ]\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/lora/worker_manager.py:167\u001b[0m, in \u001b[0;36mWorkerLoRAManager.add_dummy_lora\u001b[0;34m(self, lora_request, rank)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lora_request\u001b[38;5;241m.\u001b[39mlora_int_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlist_loras():\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lora_manager\u001b[38;5;241m.\u001b[39madd_lora(\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lora_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dummy_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_int_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_modules\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/default_cld_cv8egzp1tm3uvi738tt5bycjmm/vllm/vllm/lora/models.py:474\u001b[0m, in \u001b[0;36mLoRAModelManager.create_dummy_lora\u001b[0;34m(self, lora_id, rank, embedding_modules)\u001b[0m\n\u001b[1;32m    463\u001b[0m         lora \u001b[38;5;241m=\u001b[39m LoRALayerWeights\u001b[38;5;241m.\u001b[39mcreate_dummy_lora_weights(\n\u001b[1;32m    464\u001b[0m             module_name,\n\u001b[1;32m    465\u001b[0m             input_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    470\u001b[0m             embeddings_tensor_dim\u001b[38;5;241m=\u001b[39membeddings_tensor_dim)\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    472\u001b[0m         lora \u001b[38;5;241m=\u001b[39m LoRALayerWeights\u001b[38;5;241m.\u001b[39mcreate_dummy_lora_weights(\n\u001b[1;32m    473\u001b[0m             module_name,\n\u001b[0;32m--> 474\u001b[0m             \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_a_stacked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    475\u001b[0m             module\u001b[38;5;241m.\u001b[39mlora_b_stacked\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m    476\u001b[0m             rank,\n\u001b[1;32m    477\u001b[0m             module\u001b[38;5;241m.\u001b[39mlora_a_stacked\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    478\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    479\u001b[0m         )\n\u001b[1;32m    480\u001b[0m     lora\u001b[38;5;241m.\u001b[39moptimize()\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 23:46:45,480\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RayWorkerVllm.execute_method()\u001b[39m (pid=14408, ip=10.0.0.30, actor_id=14b645eb5080380aaf07529602000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7f0f3892d670>)\n",
      "  File \"/tmp/ray/session_2024-03-11_23-33-53_632156_5249/runtime_resources/working_dir_files/_ray_pkg_6ae155751569d798e67bbac55e5b93fd/vllm/engine/ray_utils.py\", line 37, in execute_method\n",
      "    return executor(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/ray/session_2024-03-11_23-33-53_632156_5249/runtime_resources/working_dir_files/_ray_pkg_6ae155751569d798e67bbac55e5b93fd/vllm/worker/worker.py\", line 124, in profile_num_available_blocks\n",
      "    self.model_runner.profile_run()\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/ray/session_2024-03-11_23-33-53_632156_5249/runtime_resources/working_dir_files/_ray_pkg_6ae155751569d798e67bbac55e5b93fd/vllm/worker/model_runner.py\", line 609, in profile_run\n",
      "    self.lora_manager.add_dummy_lora(dummy_lora_request,\n",
      "  File \"/tmp/ray/session_2024-03-11_23-33-53_632156_5249/runtime_resources/working_dir_files/_ray_pkg_6ae155751569d798e67bbac55e5b93fd/vllm/lora/worker_manager.py\", line 167, in add_dummy_lora\n",
      "    self._lora_manager.create_dummy_lora(lora_request.lora_int_id,\n",
      "  File \"/tmp/ray/session_2024-03-11_23-33-53_632156_5249/runtime_resources/working_dir_files/_ray_pkg_6ae155751569d798e67bbac55e5b93fd/vllm/lora/models.py\", line 474, in create_dummy_lora\n",
      "    module.lora_a_stacked.shape[-1],\n",
      "AttributeError: 'tuple' object has no attribute 'shape'\n",
      "2024-03-11 23:46:45,481\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RayWorkerVllm.execute_method()\u001b[39m (pid=14323, ip=10.0.0.30, actor_id=bbfff576a85a63df7907e80602000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7f1932f33670>)\n",
      "  File \"/tmp/ray/session_2024-03-11_23-33-53_632156_5249/runtime_resources/working_dir_files/_ray_pkg_6ae155751569d798e67bbac55e5b93fd/vllm/engine/ray_utils.py\", line 37, in execute_method\n",
      "    return executor(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/ray/session_2024-03-11_23-33-53_632156_5249/runtime_resources/working_dir_files/_ray_pkg_6ae155751569d798e67bbac55e5b93fd/vllm/worker/worker.py\", line 124, in profile_num_available_blocks\n",
      "    self.model_runner.profile_run()\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/ray/session_2024-03-11_23-33-53_632156_5249/runtime_resources/working_dir_files/_ray_pkg_6ae155751569d798e67bbac55e5b93fd/vllm/worker/model_runner.py\", line 609, in profile_run\n",
      "    self.lora_manager.add_dummy_lora(dummy_lora_request,\n",
      "  File \"/tmp/ray/session_2024-03-11_23-33-53_632156_5249/runtime_resources/working_dir_files/_ray_pkg_6ae155751569d798e67bbac55e5b93fd/vllm/lora/worker_manager.py\", line 167, in add_dummy_lora\n",
      "    self._lora_manager.create_dummy_lora(lora_request.lora_int_id,\n",
      "  File \"/tmp/ray/session_2024-03-11_23-33-53_632156_5249/runtime_resources/working_dir_files/_ray_pkg_6ae155751569d798e67bbac55e5b93fd/vllm/lora/models.py\", line 474, in create_dummy_lora\n",
      "    module.lora_a_stacked.shape[-1],\n",
      "AttributeError: 'tuple' object has no attribute 'shape'\n",
      "2024-03-11 23:46:45,482\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RayWorkerVllm.execute_method()\u001b[39m (pid=14240, ip=10.0.0.30, actor_id=4154f0670c3d27411a3371cc02000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7f273e5916a0>)\n",
      "  File \"/tmp/ray/session_2024-03-11_23-33-53_632156_5249/runtime_resources/working_dir_files/_ray_pkg_6ae155751569d798e67bbac55e5b93fd/vllm/engine/ray_utils.py\", line 37, in execute_method\n",
      "    return executor(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/ray/session_2024-03-11_23-33-53_632156_5249/runtime_resources/working_dir_files/_ray_pkg_6ae155751569d798e67bbac55e5b93fd/vllm/worker/worker.py\", line 124, in profile_num_available_blocks\n",
      "    self.model_runner.profile_run()\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/tmp/ray/session_2024-03-11_23-33-53_632156_5249/runtime_resources/working_dir_files/_ray_pkg_6ae155751569d798e67bbac55e5b93fd/vllm/worker/model_runner.py\", line 609, in profile_run\n",
      "    self.lora_manager.add_dummy_lora(dummy_lora_request,\n",
      "  File \"/tmp/ray/session_2024-03-11_23-33-53_632156_5249/runtime_resources/working_dir_files/_ray_pkg_6ae155751569d798e67bbac55e5b93fd/vllm/lora/worker_manager.py\", line 167, in add_dummy_lora\n",
      "    self._lora_manager.create_dummy_lora(lora_request.lora_int_id,\n",
      "  File \"/tmp/ray/session_2024-03-11_23-33-53_632156_5249/runtime_resources/working_dir_files/_ray_pkg_6ae155751569d798e67bbac55e5b93fd/vllm/lora/models.py\", line 474, in create_dummy_lora\n",
      "    module.lora_a_stacked.shape[-1],\n",
      "AttributeError: 'tuple' object has no attribute 'shape'\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"bigcode/starcoder\", tensor_parallel_size=4, dtype=\"float16\", enable_lora=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
