{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_directions = torch.load('directions.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-20 05:52:57 llm_engine.py:100] Initializing an LLM engine (v0.4.1) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=Falsequantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-20 05:52:57 utils.py:620] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-20 05:52:57 selector.py:77] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 05-20 05:52:57 selector.py:33] Using XFormers backend.\n",
      "INFO 05-20 05:52:59 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 05-20 05:53:03 model_runner.py:179] Loading model weights took 13.4966 GB\n",
      "INFO 05-20 05:53:08 gpu_executor.py:123] # GPU blocks: 11270, # CPU blocks: 2048\n",
      "INFO 05-20 05:53:09 model_runner.py:903] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-20 05:53:09 model_runner.py:907] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-20 05:53:14 model_runner.py:984] Graph capturing finished in 4 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"mistralai/Mistral-7B-Instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.control_vectors.request import ControlVectorRequest\n",
    "from vllm.control_vectors.control import ControlVector\n",
    "\n",
    "\n",
    "cv = ControlVector(model_type=\"mistral\", directions=loaded_directions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "     \"I can't feel my face\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/3 [03:29<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before expand tensor([[[-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140]]],\n",
      "       device='cuda:0')\n",
      "after expand tensor([[[-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140]],\n",
      "\n",
      "        [[-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140]],\n",
      "\n",
      "        [[-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140]],\n",
      "\n",
      "        [[-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140]],\n",
      "\n",
      "        [[-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140]]],\n",
      "       device='cuda:0')\n",
      "hidden shape torch.Size([8, 4096])\n",
      "positions 1\n",
      "final control  tensor([[[-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         ...,\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140]],\n",
      "\n",
      "        [[-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         ...,\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140]],\n",
      "\n",
      "        [[-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         ...,\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         ...,\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140]],\n",
      "\n",
      "        [[-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         ...,\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140]],\n",
      "\n",
      "        [[-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         ...,\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140],\n",
      "         [-0.0224,  0.0100, -0.0178,  ...,  0.0042,  0.0138,  0.0140]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [8, 4096] doesn't match the broadcast shape [8, 8, 4096]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/workspace/vllm/ray_test_cv.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bremote-server/workspace/vllm/ray_test_cv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m outputs \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bremote-server/workspace/vllm/ray_test_cv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     prompts,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bremote-server/workspace/vllm/ray_test_cv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     sampling_params,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bremote-server/workspace/vllm/ray_test_cv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     control_vector_request\u001b[39m=\u001b[39;49mControlVectorRequest(\u001b[39m'\u001b[39;49m\u001b[39mhigh\u001b[39;49m\u001b[39m'\u001b[39;49m, control_vector\u001b[39m=\u001b[39;49mcv, coefficient\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bremote-server/workspace/vllm/ray_test_cv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m )\n",
      "File \u001b[0;32m/workspace/vllm/vllm/entrypoints/llm.py:217\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, control_vector_request, multi_modal_data)\u001b[0m\n\u001b[1;32m    202\u001b[0m     token_ids \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m prompt_token_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m prompt_token_ids[\n\u001b[1;32m    203\u001b[0m         i]\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_request(\n\u001b[1;32m    205\u001b[0m         prompt,\n\u001b[1;32m    206\u001b[0m         sampling_params[i]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[39mif\u001b[39;00m multi_modal_data \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    216\u001b[0m     )\n\u001b[0;32m--> 217\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_engine(use_tqdm)\n",
      "File \u001b[0;32m/workspace/vllm/vllm/entrypoints/llm.py:247\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    245\u001b[0m outputs: List[RequestOutput] \u001b[39m=\u001b[39m []\n\u001b[1;32m    246\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_engine\u001b[39m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 247\u001b[0m     step_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_engine\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    248\u001b[0m     \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m step_outputs:\n\u001b[1;32m    249\u001b[0m         \u001b[39mif\u001b[39;00m output\u001b[39m.\u001b[39mfinished:\n",
      "File \u001b[0;32m/workspace/vllm/vllm/engine/llm_engine.py:588\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    585\u001b[0m seq_group_metadata_list, scheduler_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mschedule()\n\u001b[1;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m scheduler_outputs\u001b[39m.\u001b[39mis_empty():\n\u001b[0;32m--> 588\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_executor\u001b[39m.\u001b[39;49mexecute_model(\n\u001b[1;32m    589\u001b[0m         seq_group_metadata_list\u001b[39m=\u001b[39;49mseq_group_metadata_list,\n\u001b[1;32m    590\u001b[0m         blocks_to_swap_in\u001b[39m=\u001b[39;49mscheduler_outputs\u001b[39m.\u001b[39;49mblocks_to_swap_in,\n\u001b[1;32m    591\u001b[0m         blocks_to_swap_out\u001b[39m=\u001b[39;49mscheduler_outputs\u001b[39m.\u001b[39;49mblocks_to_swap_out,\n\u001b[1;32m    592\u001b[0m         blocks_to_copy\u001b[39m=\u001b[39;49mscheduler_outputs\u001b[39m.\u001b[39;49mblocks_to_copy,\n\u001b[1;32m    593\u001b[0m         num_lookahead_slots\u001b[39m=\u001b[39;49mscheduler_outputs\u001b[39m.\u001b[39;49mnum_lookahead_slots)\n\u001b[1;32m    594\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    595\u001b[0m     output \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/workspace/vllm/vllm/executor/gpu_executor.py:136\u001b[0m, in \u001b[0;36mGPUExecutor.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy, num_lookahead_slots)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecute_model\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    130\u001b[0m     seq_group_metadata_list: List[SequenceGroupMetadata],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m     num_lookahead_slots: \u001b[39mint\u001b[39m,\n\u001b[1;32m    135\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[SamplerOutput]:\n\u001b[0;32m--> 136\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdriver_worker\u001b[39m.\u001b[39;49mexecute_model(\n\u001b[1;32m    137\u001b[0m         seq_group_metadata_list\u001b[39m=\u001b[39;49mseq_group_metadata_list,\n\u001b[1;32m    138\u001b[0m         blocks_to_swap_in\u001b[39m=\u001b[39;49mblocks_to_swap_in,\n\u001b[1;32m    139\u001b[0m         blocks_to_swap_out\u001b[39m=\u001b[39;49mblocks_to_swap_out,\n\u001b[1;32m    140\u001b[0m         blocks_to_copy\u001b[39m=\u001b[39;49mblocks_to_copy,\n\u001b[1;32m    141\u001b[0m         num_lookahead_slots\u001b[39m=\u001b[39;49mnum_lookahead_slots,\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspace/vllm/vllm/worker/worker.py:252\u001b[0m, in \u001b[0;36mWorker.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy, num_lookahead_slots)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mif\u001b[39;00m num_seq_groups \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    250\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[0;32m--> 252\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_runner\u001b[39m.\u001b[39;49mexecute_model(seq_group_metadata_list,\n\u001b[1;32m    253\u001b[0m                                          \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpu_cache)\n\u001b[1;32m    255\u001b[0m \u001b[39m# Worker only supports single-step execution. Wrap the output in a list\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m# to conform to interface.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39mreturn\u001b[39;00m [output]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspace/vllm/vllm/worker/model_runner.py:767\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, seq_group_metadata_list, kv_caches)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39madd_control_vector_request(control_vector_request)\n\u001b[1;32m    765\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mset_active_control_vector_request(control_vector_request)\n\u001b[0;32m--> 767\u001b[0m hidden_states \u001b[39m=\u001b[39m model_executable(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mexecute_model_kwargs)\n\u001b[1;32m    769\u001b[0m \u001b[39m# Compute the logits.\u001b[39;00m\n\u001b[1;32m    770\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcompute_logits(hidden_states, sampling_metadata)\n",
      "File \u001b[0;32m/workspace/vllm/vllm/control_vectors/models.py:103\u001b[0m, in \u001b[0;36mControlVectorModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/vllm/vllm/model_executor/models/llama.py:364\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    358\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    359\u001b[0m     input_ids: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m     attn_metadata: AttentionMetadata,\n\u001b[1;32m    363\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 364\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_ids, positions, kv_caches,\n\u001b[1;32m    365\u001b[0m                                attn_metadata)\n\u001b[1;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/vllm/vllm/model_executor/models/llama.py:291\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, inputs_embeds)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers)):\n\u001b[1;32m    290\u001b[0m     layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i]\n\u001b[0;32m--> 291\u001b[0m     hidden_states, residual \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    292\u001b[0m         positions,\n\u001b[1;32m    293\u001b[0m         hidden_states,\n\u001b[1;32m    294\u001b[0m         kv_caches[i],\n\u001b[1;32m    295\u001b[0m         attn_metadata,\n\u001b[1;32m    296\u001b[0m         residual,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m hidden_states, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(hidden_states, residual)\n\u001b[1;32m    299\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/vllm/vllm/model_executor/models/llama.py:243\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata, residual)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n\u001b[1;32m    241\u001b[0m hidden_states, residual \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm(\n\u001b[1;32m    242\u001b[0m     hidden_states, residual)\n\u001b[0;32m--> 243\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states, positions)\n\u001b[1;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states, residual\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/vllm/vllm/control_vectors/layers.py:29\u001b[0m, in \u001b[0;36mMLPWithControlVector.forward\u001b[0;34m(self, hidden_states, positions)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, positions):\n\u001b[0;32m---> 29\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_layer\u001b[39m.\u001b[39;49mforward(hidden_states, positions)\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv_vector \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m         norm_pre \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnorm(hidden_states, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/workspace/vllm/vllm/control_vectors/layers.py:29\u001b[0m, in \u001b[0;36mMLPWithControlVector.forward\u001b[0;34m(self, hidden_states, positions)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, positions):\n\u001b[0;32m---> 29\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_layer\u001b[39m.\u001b[39;49mforward(hidden_states, positions)\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv_vector \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m         norm_pre \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnorm(hidden_states, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "    \u001b[0;31m[... skipping similar frames: MLPWithControlVector.forward at line 29 (13 times)]\u001b[0m\n",
      "File \u001b[0;32m/workspace/vllm/vllm/control_vectors/layers.py:29\u001b[0m, in \u001b[0;36mMLPWithControlVector.forward\u001b[0;34m(self, hidden_states, positions)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, positions):\n\u001b[0;32m---> 29\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_layer\u001b[39m.\u001b[39;49mforward(hidden_states, positions)\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv_vector \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m         norm_pre \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnorm(hidden_states, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/workspace/vllm/vllm/control_vectors/layers.py:58\u001b[0m, in \u001b[0;36mMLPWithControlVector.forward\u001b[0;34m(self, hidden_states, positions)\u001b[0m\n\u001b[1;32m     55\u001b[0m control \u001b[39m=\u001b[39m control \u001b[39m*\u001b[39m mask\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfinal control \u001b[39m\u001b[39m\"\u001b[39m, control)\n\u001b[0;32m---> 58\u001b[0m hidden_states \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m control\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize:\n\u001b[1;32m     61\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m*\u001b[39m (norm_pre \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39mnorm(hidden_states, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [8, 4096] doesn't match the broadcast shape [8, 8, 4096]"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs = llm.generate(\n",
    "    prompts,\n",
    "    sampling_params,\n",
    "    control_vector_request=ControlVectorRequest('high', control_vector=cv, coefficient=0.1)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=8, prompt=\"I can't feel my face\", prompt_token_ids=[1, 315, 541, 28742, 28707, 1601, 586, 2105], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\", \\n\\nI'm a 16 year old girl, and I've been having a lot of problems with my self esteem lately. I've been feeling like I'm not good enough, and I've been struggling to find my place in the world. I've been trying to find ways to boost my self esteem, but nothing seems to be working. I've tried therapy, meditation, and even talking to my friends, but nothing seems to\", token_ids=[28725, 28705, 13, 13, 28737, 28742, 28719, 264, 28705, 28740, 28784, 879, 1571, 2746, 28725, 304, 315, 28742, 333, 750, 2461, 264, 2055, 302, 4418, 395, 586, 1008, 7021, 366, 22628, 28723, 315, 28742, 333, 750, 4622, 737, 315, 28742, 28719, 459, 1179, 2066, 28725, 304, 315, 28742, 333, 750, 15829, 298, 1300, 586, 1633, 297, 272, 1526, 28723, 315, 28742, 333, 750, 2942, 298, 1300, 4342, 298, 10974, 586, 1008, 7021, 366, 28725, 562, 2511, 3969, 298, 347, 2739, 28723, 315, 28742, 333, 3851, 12238, 28725, 23976, 28725, 304, 1019, 4434, 298, 586, 3282, 28725, 562, 2511, 3969, 298], cumulative_logprob=-72.09254857861401, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1716184837.555628, last_token_time=1716184837.555628, first_scheduled_time=1716184837.5593646, first_token_time=1716184837.7521715, time_in_queue=0.0037364959716796875, finished_time=1716184840.531063), lora_request=None)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
