{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_directions = torch.load('directions.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-18 04:11:34 llm_engine.py:100] Initializing an LLM engine (v0.4.1) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=Falsequantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-18 04:11:34 utils.py:620] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 06-18 04:11:35 selector.py:77] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 06-18 04:11:35 selector.py:33] Using XFormers backend.\n",
      "INFO 06-18 04:11:37 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 06-18 04:11:41 model_runner.py:179] Loading model weights took 13.4966 GB\n",
      "INFO 06-18 04:11:45 gpu_executor.py:123] # GPU blocks: 11270, # CPU blocks: 2048\n",
      "INFO 06-18 04:11:47 model_runner.py:903] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-18 04:11:47 model_runner.py:907] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-18 04:11:51 model_runner.py:984] Graph capturing finished in 4 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"mistralai/Mistral-7B-Instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.control_vectors.request import ControlVectorRequest\n",
    "from vllm.control_vectors.control import ControlVector\n",
    "\n",
    "\n",
    "cv = ControlVector(model_type=\"mistral\", directions=loaded_directions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "     \"bees\",\n",
    "     \"bees\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [20:10<00:00, 605.29s/it] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs = llm.generate(\n",
    "    prompts,\n",
    "    sampling_params,\n",
    "    control_vector_request=ControlVectorRequest('high', control_vector=cv, coefficient=0.1)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=4, prompt='bees', prompt_token_ids=[1, 347, 274], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='wax\\n\\n# Beeswax\\n\\nBeeswax is a natural wax produced by honey bees. It is a yellowish-white, opaque, waxy substance that is solid at room temperature. Beeswax is used in a variety of applications, including candle making, cosmetics, and as a protective coating for various surfaces.\\n\\nBeeswax is produced by the glands of the honey bee, specifically the wax glands located', token_ids=[28727, 897, 13, 13, 28771, 1739, 274, 28727, 897, 13, 13, 3574, 274, 28727, 897, 349, 264, 4229, 26627, 6763, 486, 18411, 347, 274, 28723, 661, 349, 264, 9684, 789, 28733, 10635, 28725, 931, 13768, 28725, 275, 22720, 18677, 369, 349, 6861, 438, 2003, 7641, 28723, 1739, 274, 28727, 897, 349, 1307, 297, 264, 6677, 302, 8429, 28725, 2490, 26000, 2492, 28725, 6841, 2243, 1063, 28725, 304, 390, 264, 25162, 1001, 1077, 354, 4118, 19454, 28723, 13, 13, 3574, 274, 28727, 897, 349, 6763, 486, 272, 319, 5603, 302, 272, 18411, 347, 28706, 28725, 10107, 272, 26627, 319, 5603, 5651], cumulative_logprob=-46.55566542610222, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1718683944.1336915, last_token_time=1718683944.1336915, first_scheduled_time=1718683944.138552, first_token_time=1718683944.216643, time_in_queue=0.004860401153564453, finished_time=1718683947.140039), lora_request=None),\n",
       " RequestOutput(request_id=5, prompt='bees', prompt_token_ids=[1, 347, 274], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\" Holy shit, I'm so fucking high right now. I feel like I'm floating on a cloud of pure euphoria. The colors are so vivid and everything is just so fucking surreal. I feel like I'm in a dream. I can't even describe how good this feels. I'm so fucking happy right now. I don't even care about anything else. I just want to keep feeling this good forever. I don't even know how\", token_ids=[12830, 5492, 28725, 315, 28742, 28719, 579, 7714, 1486, 1103, 1055, 28723, 315, 1601, 737, 315, 28742, 28719, 15689, 356, 264, 6945, 302, 7972, 317, 715, 1752, 515, 28723, 415, 9304, 460, 579, 24954, 304, 2905, 349, 776, 579, 7714, 1147, 6487, 28723, 315, 1601, 737, 315, 28742, 28719, 297, 264, 4999, 28723, 315, 541, 28742, 28707, 1019, 6685, 910, 1179, 456, 8315, 28723, 315, 28742, 28719, 579, 7714, 4610, 1103, 1055, 28723, 315, 949, 28742, 28707, 1019, 1656, 684, 2424, 1112, 28723, 315, 776, 947, 298, 1840, 4622, 456, 1179, 10739, 28723, 315, 949, 28742, 28707, 1019, 873, 910], cumulative_logprob=-50.39027356733558, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1718683944.1341236, last_token_time=1718683944.1341236, first_scheduled_time=1718683944.138552, first_token_time=1718683944.216643, time_in_queue=0.004428386688232422, finished_time=1718683947.1400487), lora_request=None)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
